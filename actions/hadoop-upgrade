#!/bin/bash
set -e
export SAVEPATH=$PATH
. /etc/environment
export PATH=$PATH:$SAVEPATH
export JAVA_HOME

current_hadoop_ver=`/usr/lib/hadoop/bin/hadoop version|head -n1|awk '{print $2}'`
config_hadoop_ver=`config-get hadoop_version`
new_hadoop_ver=${1:-`action-get version`}
cpu_arch=`lscpu|grep -i arch|awk '{print $2}'`
prepare=${2:-`action-get prepare`}
postupgrade=${3:-`action-get postupgrade`}
standalone=$(charms.reactive is_state leadership.set.ha-initialized && echo 'False' || echo 'True')
forceupgrade=False

if jps | grep -i namenode ; then
        is_namenode="true"
        chlp unitdata set is.namenode true
else
        is_namenode="false"
fi

if jps | grep -i datanode ; then
        is_datanode="true"
        chlp unitdata set is.datanode true
else
        is_datanode="false"
fi

function init_procs () {
        # stop or start hadoop procs (jps procs)
        if [ $1 == "stop" ] ; then
                hadoop_procs=$(pgrep -alf java | awk -F. '{print $NF}')
        fi
        for proc in $hadoop_procs; do
                case $proc in
                        namenode)
                                user="hdfs"
                                if [ $1 == "restart" ] ; then
                                        su $user -c "hadoop-daemon.sh stop namenode"
                                        su $user -c "hadoop-daemon.sh start namenode"
                                elif [ $1 == "start" ] && [ "$postupgrade" == "" ] ; then
                                        su $user -c "hadoop-daemon.sh start namenode -rollingUpgrade started"
                                elif [ $1 == "start" ] && [ "$postupgrade" == "rollback" ] ; then
                                        su $user -c "hadoop-daemon.sh start namenode -rollingUpgrade rollback" 
                                elif [ $1 == "start" ] && [ "${postupgrade}" == "downgrade" ] ; then
                                        su hdfs -c "hadoop-daemon.sh start namenode"
                                elif [ $1 == "stop" ] ; then
                                        su hdfs -c "hadoop-daemon.sh stop namenode"
                                fi
                                ;;
                        datanode)
                                user="hdfs"
                                if [ $1 == "stop" ] ; then
                                        su $user -c "hdfs dfsadmin -shutdownDatanode localhost:50020 upgrade"
                                else
                                        su $user -c "hadoop-daemon.sh start datanode"
                                fi
                                ;;
                        nodemanager|resourcemanager)
                                user="yarn"
                                if [ $1 == "stop" ] ; then
                                        su $user -c "yarn-daemon.sh stop $proc" 
                                else
                                        su $user -c "yarn-daemon.sh start $proc" 
                                fi
                                ;;
                        jobhistoryserver)
                                user="mapred"
                                if [ $1 == "stop" ] ; then
                                        su $user -c "mr-jobhistory-daemon.sh stop historyserver"
                                else
                                        su $user -c "mr-jobhistory-daemon.sh start historyserver"
                                fi
                                ;;
               esac
        done
        sleep 2
}

if [ "${postupgrade}" == "finalize" ] ; then
        if [ $is_namenode == "false" ] ; then
                action-fail "Namenode process not detected - please run this action on both namenodes"
                exit 0
        fi
        su hdfs -c "hdfs dfsadmin -finalizeUpgrade"
        su hdfs -c "hdfs dfsadmin -rollingUpgrade finalize"
        if [ "$?" == 0 ] ; then
                action-set finalized="true"
        else
                action-set finalized="false"
        fi
        init_procs restart
        action-set result="Ready - upgrade finalized, downgrade or rollback no longer available"
        exit 0
fi

if ! [ "$new_hadoop_ver" == "$config_hadoop_ver" ] ; then
        action-fail "Version specified does not match configured version"
        exit 0
fi

if [ "$new_hadoop_ver" == "$current_hadoop_ver" ] ; then
        action-fail "Same version already installed"
        exit 0
fi

if [ "$prepare" == "True" ] ; then
        if [ "$standalone" == "True" ]; then
            su hdfs -c "hdfs dfsadmin -safemode enter"
        fi
        su hdfs -c "hdfs dfsadmin -rollingUpgrade prepare"
        action-set result="Upgrade image started"
        exit 0
fi

if [[ "${forceupgrade}" != "True" && "$standalone" != "True" ]] ; then
        # check upgrade, if not ready / prepared set failure message and exit
        if ! su hdfs -c "hdfs dfsadmin -rollingUpgrade query" | grep -qi "Proceed with rolling upgrade"; then
                action-fail "Rolling upgrade has not been prepared, see UPGRADING-HADOOP.md"
                exit 0
        fi
fi

if [ "${postupgrade}" == "downgrade" ] ; then
        if [ -d /usr/lib/hadoop-${new_hadoop_ver} ] ; then
                rm -f /usr/lib/hadoop
                ln -s /usr/lib/hadoop-${new_hadoop_ver} /usr/lib/hadoop
                if [ -d /usr/lib/hadoop-${current_hadoop_ver}/logs ] ; then
                        mv /usr/lib/hadoop-${current_hadoop_ver}/logs /usr/lib/hadoop/
                fi
                action-set newhadoop.downgrade="successfully downgraded, ready for finalize"
                juju-log "Ready - downgrade to ${new_hadoop_ver} complete, ready for finalize"
                init_procs stop
                init_procs start
                hooks/update-status
                exit 0
        else
                if [ "${forceupgrade}" == "False" ] ; then
                        action-fail "previous version not found, cannot downgrade"
                        juju-log "previous version not found, aborting"
                        exit 0
                fi
        fi
fi

if [ "${postupgrade}" == "disabled_rollback" ] ; then
        if is-leader && [ ${is_namenode} == "true" ] ; then
                su hdfs -c "stop-dfs.sh"
        fi
        if [ -d /usr/lib/hadoop-${new_hadoop_ver} ] ; then
                rm /usr/lib/hadoop
                ln -s /usr/lib/hadoop-${new_hadoop_ver} /usr/lib/hadoop
                if [ -d /usr/lib/hadoop-${current_hadoop_ver}/logs ] ; then
                        mv /usr/lib/hadoop-${current_hadoop_ver}/logs /usr/lib/hadoop/
                fi
                action-set newhadoop.rollback="successfully rolled back"
                juju-log "Ready - rollback to ${new_hadoop_ver} complete - ready for rollback_finalize"
                exit 0
        else
                action-set newhadoop.rollback="previous version not found, unpacking..."
                juju-log "previous version not found, unpacking..."
        fi
fi

if [ "${postupgrade}" == "disabled_rollback_finalize" ] ; then
        is_namenode=`chlp unitdata get is.namenode`
        if [ ${is_namenode} == "true" ] ; then
                if is-leader ; then
                        su hdfs -c "hadoop-daemon.sh start namenode -rollingUpgrade rollback"
                        exit 0
                else
                        sleep 60
                        su hdfs -c "hdfs namenode -bootstrapStandby"
                        su hdfs -c "start-dfs.sh -rollback"
                        exit 0
                fi
        fi
fi


juju-log "Fetching hadoop-${new_hadoop_ver}-${cpu_arch}"
juju-resources fetch hadoop-${new_hadoop_ver}-${cpu_arch}
if [ ! $? -eq 0 ] ; then
        action-fail "Failed to fetch hadoop-${new_hadoop_ver}-${cpu_arch} binary"
        exit 0
fi

new_hadoop_path=`juju-resources resource_path hadoop-${new_hadoop_ver}-${cpu_arch}`
if [ -h /usr/lib/hadoop ] ; then
       rm /usr/lib/hadoop
fi

if [ -e /usr/lib/hadoop ]; then
    mv /usr/lib/hadoop/ /usr/lib/hadoop-${current_hadoop_ver}
fi
ln -s /usr/lib/hadoop-${current_hadoop_ver}/ /usr/lib/hadoop
current_hadoop_path=hadoop-${current_hadoop_ver}

juju-log "Extracting hadoop-${new_hadoop_ver}-${cpu_arch}"
tar -zxf ${new_hadoop_path} -C /usr/lib/
if [ $? -eq 0 ] ; then
        if [ -h /usr/lib/hadoop ] ; then
                rm /usr/lib/hadoop
        fi
        ln -s /usr/lib/hadoop-${new_hadoop_ver} /usr/lib/hadoop
fi
if [ -d ${current_hadoop_path}/logs ] ; then
        mv ${current_hadoop_path}/logs ${new_hadoop_path}/
fi
# set hadoop.version in unitdata
chlp unitdata set hadoop.version ${new_hadoop_ver}

if grep hadoop-lzo-${cpu_arch} resources.yaml ; then
        juju-log "Lzo found for ${cpu_arch}, installing"
        juju-resources fetch hadoop-lzo-${cpu_arch}
        if [ ! $? -eq 0 ] ; then
                action-fail "Failed to fetch hadoop-lzo-${cpu_arch} binary"
        fi
        lzo_path=`juju-resources resource_path hadoop-lzo-${cpu_arch}`
        tar -zxf ${lzo_path} -C /usr/lib/hadoop/
fi

action-set result="complete"
juju-log "Hadoop version ${new_hadoop_ver} installed"
init_procs stop
init_procs start
hooks/update-status  # ensure new spec is sent out / verified
